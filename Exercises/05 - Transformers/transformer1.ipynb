{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3YaGFE03v1"
   },
   "source": [
    "# ðŸ“š Exercise Session - Week 3: Attention + Transformers\n",
    "**Main Topics**: Attention & Transformers for Sequence-to-Sequence Modeling\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This weekâ€™s session dives into Transformers for sequence-to-sequence (Seq2Seq) tasks, with a special focus on machine translation and attention visualization. By the end, you will have built and trained a Transformer model capable of translating text from one language to anotherâ€”and youâ€™ll see how attention helps it learn alignments across tokens.\n",
    "\n",
    "1. [**TASK A:** Transformer Implementation](#Task-A:-Transformer-Implementation)\n",
    "- Build a BPE tokenizer\n",
    "- Implement a Transformer encoder-decoder model using PyTorch\n",
    "\n",
    "2. [**TASK B:** Train a Machine Translation Model](#Task-B:-Train-a-Machine-Translation-Model)\n",
    "- Use Transformer from Task A to train a machine translation model\n",
    "- Visualize the cross-attention weights\n",
    "\n",
    "3. **Optional Extensions** \n",
    "- Learning Rate Scheduler: Try adding a scheduler (like Warmup or ReduceLROnPlateau) to potentially improve convergence.\n",
    "- Periodic Validation: Every `m` batches or at epochâ€™s end, evaluate on a validation set to track loss or metrics like BLEU.\n",
    "- Use PyTorch `DataLoader`\n",
    "\n",
    "**Tips & Hints**\n",
    "- **Overfitting on a Single Example**: If your model cannot easily learn one sample to near-perfect accuracy, it often indicates an implementation bug or mismatch in shapes/masks.\n",
    "- **Masking**: Pay careful attention to causal masks in the decoder, ensuring the model does not see future tokens.\n",
    "- **Debugging**: Print shapes and partial outputs, or watch the attention scores to confirm they behave as expected.\n",
    "\n",
    "\n",
    "> **By the end of the session you will be able to:**\n",
    "> - âœ…  Implement an encoder-decoder Transformer model using PyTorch\n",
    "> - âœ…  Train your model on a machine translation corpus\n",
    "> - âœ…  Understand attention mechanism within the Transformer architecture\n",
    "> - âœ…  Be more interested in NLP ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yDoUnLJKoIK"
   },
   "source": [
    "## Task A: Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Reminder**\n",
    "We will be implementing the following encoder-decoder transformer architecture following the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper!\n",
    "\n",
    "![Transformer Architecture](transformer_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kH-A3gdHF7o"
   },
   "source": [
    "In this part, you will implement an encoder-decoder Transformer model using [Pytorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T14:42:37.478843Z",
     "iopub.status.busy": "2024-02-22T14:42:37.478185Z",
     "iopub.status.idle": "2024-02-22T14:42:50.715331Z",
     "shell.execute_reply": "2024-02-22T14:42:50.714317Z",
     "shell.execute_reply.started": "2024-02-22T14:42:37.478777Z"
    },
    "id": "vFN2GC6PX1_P",
    "outputId": "aa984dba-d338-4f62-eaba-1c8006dc623e",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-09-26T09:20:12.814694Z",
     "start_time": "2025-09-26T09:20:11.685835Z"
    }
   },
   "source": [
    "!pip install datasets\n",
    "!pip install seaborn"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (4.1.1)\r\n",
      "Requirement already satisfied: filelock in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (3.19.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (2.3.3)\r\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (21.0.0)\r\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (0.4.0)\r\n",
      "Requirement already satisfied: pandas in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (2.3.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets)\r\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\r\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (0.34.4)\r\n",
      "Requirement already satisfied: packaging in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: idna>=2.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\r\n",
      "Installing collected packages: multiprocess\r\n",
      "  Attempting uninstall: multiprocess\r\n",
      "    Found existing installation: multiprocess 0.70.18\r\n",
      "    Uninstalling multiprocess-0.70.18:\r\n",
      "      Successfully uninstalled multiprocess-0.70.18\r\n",
      "Successfully installed multiprocess-0.70.16\r\n",
      "Requirement already satisfied: seaborn in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from seaborn) (2.3.3)\r\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from seaborn) (2.3.2)\r\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from seaborn) (3.10.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:20:13.678555Z",
     "start_time": "2025-09-26T09:20:12.823670Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:51:50.721932Z",
     "start_time": "2025-09-26T09:51:50.716284Z"
    }
   },
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # These layers transform the input embeddings to queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final projection after concatenating heads\n",
    "        self.out_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query, key, value: [batch_size, seq_len, embed_dim]\n",
    "        mask: [batch_size, 1, seq_len, seq_len] or None (optional)\n",
    "\n",
    "        returns:\n",
    "          - output: [batch_size, seq_len, embed_dim]\n",
    "          - attn_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1) Linear projections\n",
    "        # TODO: project query, key, and value via the layers: self.query_proj, self.key_proj, self.value_proj\n",
    "        # shape after projection: [batch_size, seq_len, embed_dim]\n",
    "        Q = self.query_proj(query)\n",
    "        K = self.key_proj(key)\n",
    "        V = self.value_proj(value)\n",
    "\n",
    "        # 2) Split into multiple heads\n",
    "        # We want shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "        # TODO: reshape the Q, K, V so that we chunk embed_dim into (num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, self.num_heads, -1, self.head_dim)\n",
    "        K = K.view(batch_size, self.num_heads, -1, self.head_dim)\n",
    "        V = V.view(batch_size, self.num_heads, -1, self.head_dim)\n",
    "\n",
    "        # 3) Scaled dot-product attention\n",
    "        #    attention_scores = Q x K^T / sqrt(head_dim)\n",
    "        #    then apply optional mask (if not None)\n",
    "        #    then softmax, then dropout, then multiply by V\n",
    "\n",
    "        # TODO: compute attention_scores\n",
    "        # attention_scores shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # TODO: apply mask if given\n",
    "        # if mask is not None:\n",
    "        #     attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # TODO: normalize attention_scores\n",
    "        normalized_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # TODO: compute attention_output\n",
    "        # shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "        attention_output = torch.matmul(normalized_weights, V)\n",
    "\n",
    "        # 4) Concat heads\n",
    "        # We want shape: [batch_size, seq_len, embed_dim]\n",
    "        # (i.e. combine num_heads and head_dim back into embed_dim)\n",
    "        attention_output = attention_output.view(batch_size, -1, self.embed_dim)\n",
    "        \n",
    "        # 5) Final linear projection\n",
    "        output = self.out_proj(attention_output)\n",
    "        \n",
    "        return output, normalized_weights  # (attn_weights = normalized_weights for visualization)"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:51:54.904488Z",
     "start_time": "2025-09-26T09:51:54.900195Z"
    }
   },
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.linear1   = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2   = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.norm1     = nn.LayerNorm(embed_dim)\n",
    "        self.norm2     = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1  = nn.Dropout(dropout)\n",
    "        self.dropout2  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 1) Multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask)\n",
    "        x = x + attn_output  # residual\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 2) Feed-forward\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + ff_output  # residual\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:51:56.649530Z",
     "start_time": "2025-09-26T09:51:56.644096Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Multi-Head Attention Layers\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)   # Students already have MHA\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "\n",
    "        # Feed-Forward Layers\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "        # LayerNorms\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, tgt_seq_len, embed_dim] -- decoder input embeddings\n",
    "            enc_output: [batch_size, src_seq_len, embed_dim] -- encoder output\n",
    "            tgt_mask: mask for target self-attention (e.g., causal + padding)\n",
    "            cross_attn_mask: mask for encoder-decoder attention (padding mask for source)\n",
    "        Returns:\n",
    "            x: [batch_size, tgt_seq_len, embed_dim] -- updated decoder features\n",
    "            attn_weights: attention weights from cross-attention (for visualization, etc.)\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 1) TODO: Implement cross-attention with encoder output\n",
    "        #    - Query = x (decoder), Key/Value = enc_output\n",
    "        #    - residual connection + self.norm2\n",
    "        \n",
    "        attn_output_2, _ = self.cross_attn(x, enc_output, enc_output, mask=cross_attn_mask)\n",
    "        x = x + attn_output_2\n",
    "        x = self.norm2(x)\n",
    "        # 2) TODO: Implement feed-forward sub-layer\n",
    "        #    - pass x through self.linear1, then an activation (e.g., F.relu)\n",
    "        #    - then self.linear2\n",
    "        #    - Add residual + self.norm3\n",
    "        ffn_out = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + ffn_out\n",
    "        x = self.norm3(x)\n",
    "        \n",
    "        # TODO: return final x and the cross-attention weights\n",
    "        return x, _  # return cross-attention weights for visualization\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:51:58.768510Z",
     "start_time": "2025-09-26T09:51:58.763724Z"
    }
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        src: [batch_size, src_seq_len]\n",
    "        src_mask: (optional)\n",
    "        \"\"\"\n",
    "        x = self.embedding(src) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        # TODO: Add positional encoding to the input embeddings\n",
    "        x = x + self.pos_encoding(torch.arange(src.size(1), device=src.device).unsqueeze(0))\n",
    "        # unsqueeze to add batch dimension\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # TODO: Implement the forward pass through the encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        \n",
    "        \n",
    "        return x  # shape: [batch_size, src_seq_len, embed_dim]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: [batch_size, tgt_seq_len]\n",
    "        enc_output: [batch_size, src_seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.embedding(tgt) * math.sqrt(self.embed_dim)\n",
    "        # TODO: Add positional encoding to the input embeddings\n",
    "        x = x + self.pos_encoding(torch.arange(tgt.size(1), device=tgt.device).unsqueeze(0))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        attn_weights = None\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, enc_output, tgt_mask=tgt_mask, cross_attn_mask=cross_attn_mask)\n",
    "\n",
    "        # Final projection to vocabulary\n",
    "        logits = self.out_proj(x)  # [batch_size, tgt_seq_len, vocab_size]\n",
    "        return logits, attn_weights"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:52:02.273685Z",
     "start_time": "2025-09-26T09:52:02.268964Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(seq_len, device=None):\n",
    "    \"\"\"\n",
    "    Returns a 2D causal mask of shape [seq_len, seq_len], \n",
    "    where True means 'allowed to attend' and False means 'disallowed'.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device))\n",
    "\n",
    "def expand_causal_mask(causal_2d, batch_size, num_heads):\n",
    "    # causal_2d: shape [seq_len, seq_len]\n",
    "    # expand to [batch_size, num_heads, seq_len, seq_len]\n",
    "    causal_4d = causal_2d.unsqueeze(0).unsqueeze(0)  # => [1,1,seq_len,seq_len]\n",
    "    causal_4d = causal_4d.expand(batch_size, num_heads, causal_2d.size(0), causal_2d.size(1))\n",
    "    return causal_4d\n",
    "\n",
    "def expand_padding_mask(pad_mask_2d, num_heads):\n",
    "    # pad_mask_2d: shape [batch_size, seq_len], 1 = valid, 0 = pad\n",
    "    # step 1) Convert to bool if needed\n",
    "    pad_mask_bool = pad_mask_2d.bool()  # shape [batch_size, seq_len]\n",
    "    # step 2) unsqueeze => [batch_size, 1, 1, seq_len]\n",
    "    pad_mask_4d = pad_mask_bool.unsqueeze(1).unsqueeze(2)\n",
    "    # step 3) broadcast across the query dimension\n",
    "    batch_size, _, _, seq_len = pad_mask_4d.shape\n",
    "    pad_mask_4d = pad_mask_4d.expand(batch_size, num_heads, seq_len, seq_len)\n",
    "    return pad_mask_4d\n",
    "\n",
    "\n",
    "def build_decoder_mask(\n",
    "    pad_mask_2d: torch.Tensor, \n",
    "    num_heads: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine the target padding mask with the causal mask \n",
    "    to produce a final decoder mask of shape:\n",
    "    [batch_size, num_heads, seq_len, seq_len], \n",
    "    where True means 'allowed', False means 'masked out'.\n",
    "    \"\"\"\n",
    "    device = pad_mask_2d.device\n",
    "    batch_size, seq_len = pad_mask_2d.shape\n",
    "\n",
    "    # 1) Build the 2D causal mask\n",
    "    causal_2d = build_causal_mask(seq_len, device=device)\n",
    "\n",
    "    # 2) Expand to 4D\n",
    "    causal_4d = expand_causal_mask(causal_2d, batch_size, num_heads)\n",
    "\n",
    "    # 3) Expand the padding mask to 4D\n",
    "    pad_4d = expand_padding_mask(pad_mask_2d, num_heads)\n",
    "\n",
    "    # 4) Final mask = causal AND pad\n",
    "    final_mask = causal_4d & pad_4d  # shape [batch_size, num_heads, seq_len, seq_len]\n",
    "    return final_mask\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:52:05.790374Z",
     "start_time": "2025-09-26T09:52:05.786044Z"
    }
   },
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=512, num_heads=8, hidden_dim=2048, \n",
    "                 num_layers=6, dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder = Encoder(src_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, cross_attn_mask=None):\n",
    "\n",
    "        if src_mask is not None:\n",
    "            src_mask = src_mask[:, None, None, :]\n",
    "            src_mask = src_mask.expand(-1, -1, src_mask.size(-1), -1)\n",
    "\n",
    "        tgt_mask = build_decoder_mask(tgt_mask, num_heads=self.num_heads)\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        logits, attn_weights = self.decoder(tgt, enc_output, tgt_mask, cross_attn_mask)\n",
    "        return logits, attn_weights"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Train a Machine Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to download the French-English translation dataset using [this link](https://drive.google.com/file/d/1cPKNjpU7PiqA33GzV0yICwDjZ_0ysKjO/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:20:16.982190Z",
     "start_time": "2025-09-26T09:20:13.779296Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_from_disk(\"wmt14_fr_en\")\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:20:24.135630Z",
     "start_time": "2025-09-26T09:20:17.050449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "print(datasets.__file__)   # should point to site-packages in your environment\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardzhang/miniconda3/envs/nlp/lib/python3.12/site-packages/datasets/__init__.py\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a BPE Tokenizer\n",
    "\n",
    "> You will learn about BPE tokenization later in the semester. Here we provide you with the code necessary to train your own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:20:45.995541Z",
     "start_time": "2025-09-26T09:20:41.037078Z"
    }
   },
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Yields batches of text from the Hugging Face dataset.\n",
    "    Args:\n",
    "        dataset: a huggingface dataset split (e.g. train_dataset)\n",
    "        batch_size: how many samples per batch\n",
    "        text_column: name of the column containing the text\n",
    "    \"\"\"\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        # Extract a batch of examples\n",
    "        batch = dataset[i : i + batch_size]\n",
    "\n",
    "        # 'batch' is now a list of strings (if text_column is indeed text).\n",
    "        yield batch\n",
    "\n",
    "\n",
    "fr_training_set = [dataset[\"train\"][i]['translation']['fr'] for i in range(100_000)]\n",
    "en_training_set = [dataset[\"train\"][i]['translation']['en'] for i in range(100_000)]\n",
    "training_set = en_training_set + fr_training_set\n",
    "\n",
    "# 1) Initialize tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# 2) Train from the iterator\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(training_set, batch_size=1000),\n",
    "    vocab_size=32_000,  # Choose your vocab size\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]  # Or any set of special tokens you prefer\n",
    ")\n",
    "\n",
    "# 3) Save the tokenizer\n",
    "tokenizer.save(\"my_bytelevel_tokenizer.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:21:06.328584Z",
     "start_time": "2025-09-26T09:21:06.281696Z"
    }
   },
   "source": [
    "# Load the tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"my_bytelevel_tokenizer.json\",  # or the two files from the BPE approach\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\"\n",
    ")\n",
    "\n",
    "encoded_input = hf_tokenizer(\"The quick brown fox\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "decoded_output = hf_tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "\n",
    "print(encoded_input)\n",
    "print(decoded_output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 439, 8722, 6667, 1727,  291, 3255]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "['The', 'Ä quick', 'Ä bro', 'wn', 'Ä f', 'ox']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:21:26.334713Z",
     "start_time": "2025-09-26T09:21:26.332224Z"
    }
   },
   "source": [
    "# Training parameters\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "max_len = 256\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Model parameters\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T11:54:11.219677Z",
     "start_time": "2025-09-26T09:52:13.624474Z"
    }
   },
   "source": [
    "print(\"Initializing the Transformer model...\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Print the number of parameters\n",
    "print(f\"> Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Compute total number of batches in the training set\n",
    "num_batches = int(np.ceil(len(dataset['train']) / batch_size))\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=hf_tokenizer.pad_token_id)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# Switch model to training mode\n",
    "model.train()\n",
    "for epoch_num in range(epochs):\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        train_batch = dataset['train'][batch_idx * batch_size : (batch_idx + 1) * batch_size]['translation']\n",
    "\n",
    "        src_train = [example['fr'] for example in train_batch]\n",
    "        tgt_train = [example['en'] for example in train_batch]\n",
    "        \n",
    "        src_tokens = hf_tokenizer(\n",
    "            src_train, \n",
    "            return_tensors='pt', \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len\n",
    "        ).to(device)\n",
    "\n",
    "        tgt_tokens = hf_tokenizer(\n",
    "            tgt_train, \n",
    "            return_tensors='pt', \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len\n",
    "        ).to(device)\n",
    "        \n",
    "        \n",
    "        # 1) TODO: Prepend BOS token to target tokens\n",
    "\n",
    "        src_input = src_tokens['input_ids']\n",
    "        src_mask = src_tokens['attention_mask']\n",
    "        tgt_mask = tgt_tokens['attention_mask']\n",
    "        tgt_labels = tgt_tokens['input_ids']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        # 2) TODO: Forward pass through the Transformer\n",
    "        logits, _ = model(\n",
    "            src=src_input, \n",
    "            tgt=tgt_labels[:, :-1],  # exclude last token for input\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            cross_attn_mask=src_mask\n",
    "        )\n",
    "        # logits shape: [batch_size, tgt_seq_len - 1, vocab_size]\n",
    "        # 3) TODO: Shift labels to the right (teacher forcing)\n",
    "        \n",
    "        shifted_labels = tgt_labels[:, 1:]  # exclude first token for labels\n",
    "        # 4) TODO: Compute loss\n",
    "        loss =  loss_fn(\n",
    "            logits.reshape(-1, logits.size(-1)),  # [batch_size * (tgt_seq_len - 1), vocab_size]\n",
    "            shifted_labels.reshape(-1)  # [batch_size * (tgt_seq_len - 1)\n",
    "        )\n",
    "\n",
    "        # 5) TODO: Backpropagate and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch_num}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}\", end='\\r')\n",
    "\n",
    "    print(f\"Epoch {epoch_num} Finished! Avg Loss: {np.mean(train_losses):.4f}\")\n",
    "\n",
    "\n",
    "print(\"Saving model checkpoint...\")\n",
    "torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "print(\"Training complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Transformer model...\n",
      "> Number of parameters: 79,658,240\n",
      "Epoch 0, Batch 185/6250, Loss: 7.2078\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 62\u001B[39m\n\u001B[32m     58\u001B[39m optimizer.zero_grad()\n\u001B[32m     61\u001B[39m \u001B[38;5;66;03m# 2) TODO: Forward pass through the Transformer\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m logits, _ = model(\n\u001B[32m     63\u001B[39m     src=src_input, \n\u001B[32m     64\u001B[39m     tgt=tgt_labels[:, :-\u001B[32m1\u001B[39m],  \u001B[38;5;66;03m# exclude last token for input\u001B[39;00m\n\u001B[32m     65\u001B[39m     src_mask=src_mask,\n\u001B[32m     66\u001B[39m     tgt_mask=tgt_mask,\n\u001B[32m     67\u001B[39m     cross_attn_mask=src_mask\n\u001B[32m     68\u001B[39m )\n\u001B[32m     69\u001B[39m \u001B[38;5;66;03m# logits shape: [batch_size, tgt_seq_len - 1, vocab_size]\u001B[39;00m\n\u001B[32m     70\u001B[39m \u001B[38;5;66;03m# 3) TODO: Shift labels to the right (teacher forcing)\u001B[39;00m\n\u001B[32m     72\u001B[39m shifted_labels = tgt_labels[:, \u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# exclude first token for labels\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[35]\u001B[39m\u001B[32m, line 18\u001B[39m, in \u001B[36mTransformer.forward\u001B[39m\u001B[34m(self, src, tgt, src_mask, tgt_mask, cross_attn_mask)\u001B[39m\n\u001B[32m     15\u001B[39m tgt_mask = build_decoder_mask(tgt_mask, num_heads=\u001B[38;5;28mself\u001B[39m.num_heads)\n\u001B[32m     17\u001B[39m enc_output = \u001B[38;5;28mself\u001B[39m.encoder(src, src_mask)\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m logits, attn_weights = \u001B[38;5;28mself\u001B[39m.decoder(tgt, enc_output, tgt_mask, cross_attn_mask)\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m logits, attn_weights\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 58\u001B[39m, in \u001B[36mDecoder.forward\u001B[39m\u001B[34m(self, tgt, enc_output, tgt_mask, cross_attn_mask)\u001B[39m\n\u001B[32m     56\u001B[39m attn_weights = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers:\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m     x, attn_weights = layer(x, enc_output, tgt_mask=tgt_mask, cross_attn_mask=cross_attn_mask)\n\u001B[32m     60\u001B[39m \u001B[38;5;66;03m# Final projection to vocabulary\u001B[39;00m\n\u001B[32m     61\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.out_proj(x)  \u001B[38;5;66;03m# [batch_size, tgt_seq_len, vocab_size]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 43\u001B[39m, in \u001B[36mDecoderLayer.forward\u001B[39m\u001B[34m(self, x, enc_output, tgt_mask, cross_attn_mask)\u001B[39m\n\u001B[32m     37\u001B[39m x = \u001B[38;5;28mself\u001B[39m.norm1(x)\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# 1) TODO: Implement cross-attention with encoder output\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m#    - Query = x (decoder), Key/Value = enc_output\u001B[39;00m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m#    - residual connection + self.norm2\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m attn_output_2, _ = \u001B[38;5;28mself\u001B[39m.cross_attn(x, enc_output, enc_output, mask=cross_attn_mask)\n\u001B[32m     44\u001B[39m x = x + attn_output_2\n\u001B[32m     45\u001B[39m x = \u001B[38;5;28mself\u001B[39m.norm2(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 69\u001B[39m, in \u001B[36mMultiHeadAttention.forward\u001B[39m\u001B[34m(self, query, key, value, mask)\u001B[39m\n\u001B[32m     66\u001B[39m attention_output = attention_output.view(batch_size, -\u001B[32m1\u001B[39m, \u001B[38;5;28mself\u001B[39m.embed_dim)\n\u001B[32m     68\u001B[39m \u001B[38;5;66;03m# 5) Final linear projection\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m output = \u001B[38;5;28mself\u001B[39m.out_proj(attention_output)\n\u001B[32m     71\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output, normalized_weights\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.linear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m.weight, \u001B[38;5;28mself\u001B[39m.bias)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.lineplot(x=range(len(train_losses)), y=train_losses, marker='o', markersize=5)\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Examples\n",
    "\n",
    "Generate using greedy decoding. You will learn more about it in class later in the semester.\n",
    "\n",
    "Here is a [link for a checkpoint](https://drive.google.com/file/d/130dDwMBJGhvSEFQdHkxaU-R5IjlSbO2j/view?usp=sharing) that you can use for prediction and attention weight visualization.\n",
    "It was trained using the following configuration\n",
    "```\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4\n",
    "vocab_size = 32000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "state_dict = torch.load(\"transformer_model.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "src_val = [dataset['train'][i]['translation']['fr'] for i in range(5)]\n",
    "tgt_val = [dataset['train'][i]['translation']['en'] for i in range(5)]\n",
    "\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        # Prepare inputs\n",
    "        src = src_tokens_val['input_ids'][i].unsqueeze(0).to(device)\n",
    "        tgt = torch.tensor([[hf_tokenizer.bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "        # Greedy decode up to max_len\n",
    "        for step in range(max_len):\n",
    "            # Create tgt padding mask\n",
    "            tgt_mask = torch.ones_like(tgt).bool().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "            # Get the most probable token at the current step\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "            # Append\n",
    "            tgt = torch.cat([tgt, next_token.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # End conditions\n",
    "            if next_token.item() in [hf_tokenizer.eos_token_id, hf_tokenizer.pad_token_id]:\n",
    "                break\n",
    "\n",
    "        # Decode\n",
    "        translation = hf_tokenizer.decode(tgt[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Source:      {src_val[i]}\")\n",
    "        print(f\"  Translation: {translation}\")\n",
    "        print(f\"  Target:      {tgt_val[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLXazZQx5roi"
   },
   "source": [
    "## Analyze Cross-Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "900FaGrbMCms"
   },
   "source": [
    "- Choose one sample sentence, output attention weights for each token using heatmap\n",
    "- Which pairs of the token have the greatest attention weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:26.595566Z",
     "iopub.status.busy": "2024-02-22T15:18:26.594626Z",
     "iopub.status.idle": "2024-02-22T15:18:27.145253Z",
     "shell.execute_reply": "2024-02-22T15:18:27.144212Z",
     "shell.execute_reply.started": "2024-02-22T15:18:26.595530Z"
    },
    "id": "QQMMFUipeVkd",
    "outputId": "f5822cd3-9fdf-4af5-9333-1eef3fad1c4e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_cross_attention(\n",
    "    attn_weights, \n",
    "    source_tokens, \n",
    "    target_tokens, \n",
    "    batch_idx=0, \n",
    "    head_idx=0, \n",
    "    title=\"Cross-Attention\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize cross-attention weights for a given batch and head.\n",
    "\n",
    "    Args:\n",
    "        attn_weights: Tensor of shape [batch_size, num_heads, tgt_len, src_len]\n",
    "            Cross-attention weights from your Transformer decoder, \n",
    "            typically returned alongside logits in a (logits, attn_weights) tuple.\n",
    "        source_tokens: List of source tokens (strings) for the batch_idx sample.\n",
    "        target_tokens: List of target tokens (strings) for the batch_idx sample.\n",
    "        batch_idx: Which batch element to visualize (default=0).\n",
    "        head_idx: Which attention head to visualize (default=0).\n",
    "        title: Title for the plot.\n",
    "\n",
    "    Example Usage:\n",
    "        # Suppose attn_weights has shape [batch_size, num_heads, tgt_len, src_len]\n",
    "        # and you have the corresponding token lists for the source and target:\n",
    "        visualize_cross_attention(attn_weights, src_tokens, tgt_tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract the attention for the specified batch & head\n",
    "    #    shape: [tgt_len, src_len]\n",
    "    attn = attn_weights[batch_idx, head_idx].detach().cpu().numpy()\n",
    "\n",
    "    tgt_len, src_len = attn.shape\n",
    "\n",
    "    # 2) Plot the heatmap\n",
    "    plt.figure(figsize=(min(12, 1 + 0.5 * src_len), min(6, 1 + 0.5 * tgt_len)))\n",
    "    sns.heatmap(attn, \n",
    "                vmin=0.0, vmax=1.0, \n",
    "                cmap=\"Blues\", \n",
    "                xticklabels=source_tokens, \n",
    "                yticklabels=target_tokens, \n",
    "                cbar=True)\n",
    "\n",
    "    plt.title(f\"{title} (batch={batch_idx}, head={head_idx})\")\n",
    "    plt.xlabel(\"Source Tokens\")\n",
    "    plt.ylabel(\"Target Tokens\")\n",
    "\n",
    "    # Rotate the x-axis labels if tokens are long\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:40.105997Z",
     "iopub.status.busy": "2024-02-22T15:18:40.105643Z",
     "iopub.status.idle": "2024-02-22T15:18:40.181096Z",
     "shell.execute_reply": "2024-02-22T15:18:40.180179Z",
     "shell.execute_reply.started": "2024-02-22T15:18:40.105972Z"
    },
    "id": "HOxwg2xw52rG",
    "outputId": "f4b013bd-85e3-44ba-eba4-6656db8963cf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src_val = \"Le renard brun rapide saute par-dessus le chien paresseux.\"\n",
    "tgt_val = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the source and target\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "tgt_tokens_val = hf_tokenizer(tgt_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "print(src_tokens_val[\"input_ids\"].shape)\n",
    "print(tgt_tokens_val[\"input_ids\"].shape)\n",
    "\n",
    "# Run the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, attn_weights = model(\n",
    "        src=src_tokens_val['input_ids'],\n",
    "        tgt=tgt_tokens_val['input_ids'],\n",
    "        src_mask=src_tokens_val['attention_mask'],\n",
    "        tgt_mask=tgt_tokens_val['attention_mask']\n",
    "    )\n",
    "\n",
    "# Decode the source and target sequences\n",
    "src_tokens = hf_tokenizer.convert_ids_to_tokens(src_tokens_val[\"input_ids\"][0])\n",
    "tgt_tokens = hf_tokenizer.convert_ids_to_tokens(tgt_tokens_val[\"input_ids\"][0])\n",
    "\n",
    "# Visualize the attention weights\n",
    "for i in range(num_heads):\n",
    "    visualize_cross_attention(\n",
    "        attn_weights=attn_weights,\n",
    "        source_tokens=src_tokens,\n",
    "        target_tokens=tgt_tokens,\n",
    "        batch_idx=0,\n",
    "        head_idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpXzr36fLUtI"
   },
   "source": [
    "## Congrats! You can now train a simple machine translator by your own ;)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4480106,
     "sourceId": 7679312,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
