{
 "cells": [
  {
   "cell_type": "code",
   "id": "8e211684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:47:58.488787Z",
     "start_time": "2025-09-25T23:47:58.484343Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q: Queries (shape: [n_queries, d_k])\n",
    "    K: Keys    (shape: [n_keys, d_k])\n",
    "    V: Values  (shape: [n_keys, d_v])\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # key dimension\n",
    "    \n",
    "    # 1. Raw scores = Q @ K^T\n",
    "    scores = Q @ K.T  # alternative syntax\n",
    "    # transpose -2 and -1 because we may have batch dimensions\n",
    "    print(f\"Raw scores: {scores}\")\n",
    "    \n",
    "    # 2. Scale\n",
    "    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    print(f\"Scaled scores: {scaled_scores}\")\n",
    "    \n",
    "    # 3. Softmax along keys axis\n",
    "    weights = scaled_scores/torch.sum(scaled_scores, dim=-1, keepdim=True)\n",
    "    # dim -1 is the keys axis, keepdim to maintain shape for broadcasting\n",
    "\n",
    "    print(f\"Attention weights: {weights}\")\n",
    "    \n",
    "    # 4. Weighted sum of values\n",
    "    output = weights @ V  # shape: [n_queries, d_v]\n",
    "    print(f\"Final output: {output}\")\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Example sentence: \"He sat by the river bank\"\n",
    "words = [\"He\", \"sat\", \"river\", \"bank\"]\n",
    "\n",
    "Q = torch.tensor([[1.0, 0.0]])   # the query vector for \"bank\"\n",
    "K = torch.tensor([\n",
    "    [1.0, 0.0],  # \"He\"\n",
    "    [0.0, 1.0],  # \"sat\"\n",
    "    [1.0, 1.0],  # \"river\"\n",
    "    [1.0, 0.0],  # \"bank\"\n",
    "])\n",
    "V = torch.tensor([\n",
    "    [1.0, 0.0],  # info from \"He\"\n",
    "    [0.0, 1.0],  # info from \"sat\"\n",
    "    [0.5, 0.5],  # info from \"river\"\n",
    "    [1.0, 0.0],  # info from \"bank\"\n",
    "])\n",
    "\n",
    "print('Sentence: \"He sat by the river bank\"')\n",
    "print('Query: \"bank\" looking for relevant context\\n')\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nAttention distribution:\")\n",
    "for word, weight in zip(words, weights[0]):\n",
    "    print(f\"{word}: {weight:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"He sat by the river bank\"\n",
      "Query: \"bank\" looking for relevant context\n",
      "\n",
      "Raw scores: tensor([[1., 0., 1., 1.]])\n",
      "Scaled scores: tensor([[0.7071, 0.0000, 0.7071, 0.7071]])\n",
      "Attention weights: tensor([[0.3333, 0.0000, 0.3333, 0.3333]])\n",
      "Final output: tensor([[0.8333, 0.1667]])\n",
      "\n",
      "Attention distribution:\n",
      "He: 0.333\n",
      "sat: 0.000\n",
      "river: 0.333\n",
      "bank: 0.333\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e59f1ea01068b6ce"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
