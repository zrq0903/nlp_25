Please refer to the following materials to better understand the Seq2Seq, Attention, and Transformer model.

1. The Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/

2. How do Transformers work?: https://huggingface.co/learn/llm-course/en/chapter1/4

3. Some code to explore: `./attention.ipynb`, `./transformer1.ipynb`, `./transformer2.ipynb`   