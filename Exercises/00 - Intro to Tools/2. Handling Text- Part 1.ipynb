{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling text exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Hands on tutorial\n",
    "1. Implementing the natural language processing pipeline\n",
    "2. Solving four typical language processing tasks:\n",
    "   * Sentiment analysis\n",
    "   * Document classification\n",
    "   * Topic detection\n",
    "   * Semantic analysis\n",
    "\n",
    "\n",
    "### We will be working with four books in plain text:\n",
    "1. Moby-Dick, by Herman Melville\n",
    "2. Frankenstein; or, The Modern Prometheus, by Mary Shelley\n",
    "3. Pride and Prejudice, by Jane Austen\n",
    "4. Dracula, Bram Stoker\n",
    "\n",
    "#### To run this notebook, you need to install the following libraries:\n",
    "\n",
    "pip install nltk gensim spacy <br>\n",
    "pip install pyLDAvis  <br>\n",
    "pip install empath <br>\n",
    "pip install matplotlib <br>\n",
    "python -m spacy download en  <br>\n",
    "python -m nltk.downloader punkt <br>\n",
    "python -m nltk.downloader all-corpora <br>\n",
    "\n",
    "## Part 2: Handling text comprehension questions\n",
    "- You can test your understanding of important NLP concepts. \n",
    "\n",
    "## Part 3: Coding exercise\n",
    "- When you are done, in `Handling Text - Part 2.ipynb`, you will find additional exercises you can work on independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T07:59:05.414648Z",
     "start_time": "2025-08-29T07:59:03.212530Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os, codecs, string, random\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#NLP libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "#import pyLDAvis.gensim\n",
    "#import pyLDAvis.gensim_models\n",
    "\n",
    "# Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Scikit imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#The data\n",
    "corpus_root = 'books/'"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/richardzhang/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this tutorial we will be working with text in English. The same principles apply, but the methods cannot be applied directly to the text in unknown or other languages. We will be working mainly with the Spacy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Spacy analyzer in English"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:01:49.456049Z",
     "start_time": "2025-08-29T08:01:48.995035Z"
    }
   },
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the books"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:01:53.002624Z",
     "start_time": "2025-08-29T08:01:52.958705Z"
    }
   },
   "source": [
    "books = list()\n",
    "\n",
    "for book_file in os.listdir(corpus_root):\n",
    "    if \".txt\" in book_file:\n",
    "        print(book_file)\n",
    "        with codecs.open(os.path.join(corpus_root,book_file),encoding=\"utf8\") as f:\n",
    "            books.append(f.read())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby_Dick.txt\n",
      "Frankenstein.txt\n",
      "DRACULA.txt\n",
      "PRIDE_AND_PREJUDICE.txt\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the beginning of Pride and Prejudice"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:02:21.928366Z",
     "start_time": "2025-08-29T08:02:21.883388Z"
    }
   },
   "source": [
    "print(books[3][0:600])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIDE AND PREJUDICE\n",
      "\n",
      "By Jane Austen\n",
      "\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune, must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n",
      "of the surrounding families, that he is considered the rightful property\n",
      "of some one or other of their daughters.\n",
      "\n",
      "“My dear Mr. Bennet,” said his lady to him one day, “have you heard that\n",
      "Netherfield Park is let at last?”\n",
      "\n",
      "Mr. Bennet replied that he had not.\n",
      "\n",
      "“But it is,” returned she;\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's remove the new lines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:02:24.621449Z",
     "start_time": "2025-08-29T08:02:24.542745Z"
    }
   },
   "source": [
    "books = [\" \".join(b.split()) for b in books]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T08:02:25.919456Z",
     "start_time": "2025-08-29T08:02:25.878244Z"
    }
   },
   "source": [
    "print(books[3][0:600])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIDE AND PREJUDICE By Jane Austen Chapter 1 It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. “My dear Mr. Bennet,” said his lady to him one day, “have you heard that Netherfield Park is let at last?” Mr. Bennet replied that he had not. “But it is,” returned she; “for Mrs.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Frankenstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index all books\n",
    "book = books[1]\n",
    "\n",
    "#put in raw text, get a Spacy object\n",
    "doc = nlp(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create our own NLP pipeline with Spacy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Sentence splitting\n",
    "\n",
    "#### May sound trivial, but it's not! (e.g., U.K. or Yahoo!) Spacy uses a statistical model to generate accurate predictions. This works well out of the box for general-purpose text or web text. For social media use dedicated libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent for sent in doc.sents]\n",
    "print('Sentence 1:',sentences[0],'\\n')\n",
    "print('Sentence 2:',sentences[1],'\\n')\n",
    "print('Sentence 3:',sentences[2],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization\n",
    "\n",
    "#### The task of splitting a text into meaningful segments called tokens. We segment the sentence into words, punctuation, etc. This is done by first splitting on whitespace characters and them applying rules specific to each language. For example, \"don't\" does not contain whitespace, but should be split into two tokens, \"do\" and \"n't\", while \"U.K.\" should always remain one token and \"but,\" shouldn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will analyze the methods on an example sentence first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.'\n",
    "\n",
    "doc = nlp(example)\n",
    "\n",
    "#strings are encoded to hashes\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(example,'\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Part of speech tagging\n",
    "\n",
    "#### The model makes a prediction of which tag or label most likely applies in this context. For example, a word following \"the\" in English is most likely a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "print(example,'\\n')\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain('CCONJ'))\n",
    "print(spacy.explain('ADP'))\n",
    "print(spacy.explain('DET'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When unsure, see here for more details: http://universaldependencies.org/u/pos/all.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More detailed annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = [(token.text, token.tag_) for token in doc]\n",
    "\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain('PRP'))\n",
    "print(spacy.explain('VBP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Named entity recognition\n",
    "\n",
    "#### For example, a person, a country, a product or a book title. Spacy can recognise various types of named entities in a document. This doesn't always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Built in entity types: <br>\n",
    "\n",
    "PERSON  People, including fictional.  <br>\n",
    "NORP\tNationalities or religious or political groups. <br>\n",
    "FAC\tBuildings, airports, highways, bridges, etc. <br>\n",
    "ORG\tCompanies, agencies, institutions, etc. <br>\n",
    "GPE\tCountries, cities, states. <br>\n",
    "LOC\tNon-GPE locations, mountain ranges, bodies of water. <br>\n",
    "PRODUCT\tObjects, vehicles, foods, etc. (Not services.) <br>\n",
    "EVENT\tNamed hurricanes, battles, wars, sports events, etc. <br>\n",
    "WORK_OF_ART\tTitles of books, songs, etc. <br>\n",
    "LAW\tNamed documents made into laws. <br>\n",
    "LANGUAGE\tAny named language. <br>\n",
    "DATE\tAbsolute or relative dates or periods. <br>\n",
    "TIME\tTimes smaller than a day. <br>\n",
    "PERCENT\tPercentage, including \"%\". <br>\n",
    "MONEY\tMonetary values, including unit. <br>\n",
    "QUANTITY\tMeasurements, as of weight or distance. <br>\n",
    "ORDINAL\t\"first\", \"second\", etc. <br>\n",
    "CARDINAL\tNumerals that do not fall under another type. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example,'\\n')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out all the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words:',list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting stopwords in the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example,'\\n')\n",
    "\n",
    "stop_words = [token.text for token in doc if token.is_stop]\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes it's useful to define a custom list of stopwords. Other solutions: https://www.ranks.nl/stopwords \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Lemmatization\n",
    "\n",
    "#### A rule-based deterministic lemmatizer maps the surface form of a token to a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example,'\\n')\n",
    "for token in doc:\n",
    "    if token.text != token.lemma_:\n",
    "        print(token.text,'--->',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Chunking (shallow parsing)\n",
    "\n",
    "#### Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head -- a noun plus the words describing the noun – for example, \"the lavish green grass\" or \"the world’s largest tech fund\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example,'\\n')\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Dependancy parsing\n",
    "\n",
    "#### The terms head and child describe the words connected by an arc in the dependency tree. The type of syntactic relation that connects the child to the head can be obtain through .dep_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example,'\\n')\n",
    "\n",
    "for token in doc:\n",
    "    print('Token:',token.text,'Head:',token.head.text, 'Children:',[child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting word occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(example,'\\n')\n",
    "words = [token.text for token in doc]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()\n",
    "\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()\n",
    "\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the components together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood: the pipeline [\"tokenizer\",\"tagger\", \"parser\", \"ner\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The NLP pipeline with Spacy\n",
    "<img src=\"spacy.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you don't need a particular component of the pipeline – for example, the tagger or the parser, you can disable loading it. This can sometimes make a big difference and improve loading speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the analyzer. Typically used for data from social media (short sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vs = analyzer.polarity_scores(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sentiment score consits of four values. Neutral, positive and negative sum to one. The final score is obtained by thresholding the compound value (e.g. +/-0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example, '\\n')\n",
    "print('Negative sentiment:',vs['neg'])\n",
    "print('Neutral sentiment:',vs['neu'])\n",
    "print('Positive sentiment:',vs['pos'])\n",
    "print('Compound sentiment:',vs['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now observe the distributions of sentiment across all sentences in the book Pride and Prejudice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(books[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sent = []\n",
    "#iterate through the sentences, get polarity scores, choose a value\n",
    "[positive_sent.append(analyzer.polarity_scores(sent.text)['pos']) for sent in doc.sents]\n",
    "plt.hist(positive_sent,bins=15)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,8000])\n",
    "plt.xlabel('Positive sentiment')\n",
    "plt.ylabel('Number of sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sent = []\n",
    "[negative_sent.append(analyzer.polarity_scores(sent.text)['neg']) for sent in doc.sents]\n",
    "plt.hist(negative_sent,bins=15)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,8000])\n",
    "plt.xlabel('Negative sentiment')\n",
    "plt.ylabel('Number of sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sent = []\n",
    "[total_sent.append(analyzer.polarity_scores(sent.text)['compound']) for sent in doc.sents]\n",
    "plt.hist(total_sent,bins = 15)\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([0,8000])\n",
    "plt.xlabel('Compound sentiment')\n",
    "plt.ylabel('Number of sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [analyzer.polarity_scores(sent.text)['compound'] for sent in doc.sents]\n",
    "print('Number of positive sentences:',sum(np.array(sents)>=0.05))\n",
    "print('Number of negative sentences:',sum(np.array(sents)<=-0.05))\n",
    "print('Number of neutral sentences:',sum(np.abs(np.array(sents))<0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most of the sentences are quite neutral in sentiment. This is not the case when working with data from other sources (e.g., in social media opinions are strong and polarized (bimodal distribution))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Document classification: Can we detect paragraphs from Frankenstein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our corpus via NLTK this time\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "?PlaintextCorpusReader\n",
    "our_books = PlaintextCorpusReader(corpus_root, '.*.txt')\n",
    "print(our_books.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will segment the books into equally long chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "# Get the data\n",
    "book_id = {f:n for n,f in enumerate(our_books.fileids())} # dictionary of books\n",
    "\n",
    "chunks = list()\n",
    "chunk_class = list() # this list contains the original book of the chunk, for evaluation\n",
    "\n",
    "limit = 500 # how many chunks total\n",
    "size = 50 # how many sentences per chunk/page\n",
    "\n",
    "for f in our_books.fileids():\n",
    "    sentences = our_books.sents(f)\n",
    "    print(f,\":\")\n",
    "    print('Number of sentences:',len(sentences))\n",
    "    \n",
    "    # create chunks\n",
    "    chunks_of_sents = [x for x in get_chunks(sentences,size)] # this is a list of lists of sentences, which are a list of tokens\n",
    "    chs = list()\n",
    "    \n",
    "    # regroup so to have a list of chunks which are strings\n",
    "    for c in chunks_of_sents:\n",
    "        grouped_chunk = list()\n",
    "        for s in c:\n",
    "            grouped_chunk.extend(s)\n",
    "        chs.append(\" \".join(grouped_chunk))\n",
    "    print(\"Number of chunks:\",len(chs),'\\n')\n",
    "    \n",
    "    # filter to the limit, to have the same number of chunks per book\n",
    "    chunks.extend(chs[:limit])\n",
    "    chunk_class.extend([book_id[f] for _ in range(len(chs[:limit]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the chunks with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#initialize and specify minumum number of occurences to avoid untractable number of features\n",
    "#vectorizer = CountVectorizer(min_df = 2) if we want high frequency\n",
    "\n",
    "#create bag of words features\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "print('Number of samples:',X.toarray().shape[0])\n",
    "print('Number of features:',X.toarray().shape[1])\n",
    "\n",
    "#mask and convert to int Frankenstein\n",
    "Y = np.array(chunk_class) == 1\n",
    "Y = Y.astype(int)  \n",
    "\n",
    "#shuffle the data\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "#split into training and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit the regularized logistic regression. We will crossvalidate the regularization parameter on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "#the grid of regularization parameter \n",
    "grid = [0.01,0.1,1,10,100,1000,10000]\n",
    "\n",
    "for c in grid:\n",
    "    \n",
    "    #initialize the classifier\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',C = c)\n",
    "    \n",
    "    #crossvalidate\n",
    "    scores = cross_val_score(clf, X_train,Y_train, cv=10)\n",
    "    accs.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs)\n",
    "plt.xticks(range(len(grid)), grid)\n",
    "plt.xlabel('Regularization parameter \\n (Low - strong regularization, High - weak regularization)')\n",
    "plt.ylabel('Crossvalidation accuracy')\n",
    "plt.ylim([0.986,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train again, on the entire training set, with the chosen regularization parameter. We will test on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs',C = 10).fit(X_train,Y_train)\n",
    "\n",
    "#predict on the test set\n",
    "print('Accuracy:',clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What features characterize Frankenstein? BOW are interpretable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=clf.coef_[0]\n",
    "top_three = np.argpartition(coefs, -20)[-20:]\n",
    "\n",
    "print(np.array(vectorizer.get_feature_names_out())[top_three])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can perfectly predict paragraphs from Frankenstein! We saw issues of high dimensionality when using bag of words (more features than samples) and how regularization helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative text representation: word emdeddings, pretrained on intergraded in Spacy (300-dimensional word vectors trained on Common Crawl with GloVe.) Depending on the application, might be suitable or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example,'\\n')\n",
    "print('Embedding representation:',list((nlp(example).vector)[0:10]),'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Topic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the chunks again (into smaller chunks)\n",
    "book_id = {f:n for n,f in enumerate(our_books.fileids())} # dictionary of books\n",
    "chunks = list()\n",
    "chunk_class = list() # this list contains the original book of the chunk, for evaluation\n",
    "\n",
    "limit = 60 # how many chunks total\n",
    "size = 50 # how many sentences per chunk/page\n",
    "\n",
    "for f in our_books.fileids():\n",
    "    sentences = our_books.sents(f)\n",
    "    print(f)\n",
    "    print('Number of sentences:',len(sentences))\n",
    "    \n",
    "    # create chunks\n",
    "    chunks_of_sents = [x for x in get_chunks(sentences,size)] # this is a list of lists of sentences, which are a list of tokens\n",
    "    chs = list()\n",
    "    \n",
    "    # regroup so to have a list of chunks which are strings\n",
    "    for c in chunks_of_sents:\n",
    "        grouped_chunk = list()\n",
    "        for s in c:\n",
    "            grouped_chunk.extend(s)\n",
    "        chs.append(\" \".join(grouped_chunk))\n",
    "    print(\"Number of chunks:\",len(chs),'\\n')\n",
    "    \n",
    "    # filter to the limit, to have the same number of chunks per book\n",
    "    chunks.extend(chs[:limit])\n",
    "    chunk_class.extend([book_id[f] for _ in range(len(chs[:limit]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "processed_docs = list()\n",
    "for doc in nlp.pipe(chunks, n_process=5, batch_size=10):\n",
    "\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    ents = doc.ents  # Named entities\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "    doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "    processed_docs.append(doc)\n",
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams too\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 15 times or more).\n",
    "bigram = Phrases(docs, min_count=15)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 5\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of chunks: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "from gensim.models import LdaMulticore\n",
    "params = {'passes': 10, 'random_state': seed}\n",
    "base_models = dict()\n",
    "model = LdaMulticore(corpus=corpus, num_topics=4, id2word=dictionary, workers=6,\n",
    "                passes=params['passes'], random_state=params['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(model[corpus[0]],key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topics\n",
    "data =  pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment\n",
    "sent_to_cluster = list()\n",
    "for n,doc in enumerate(corpus):\n",
    "    if doc:\n",
    "        cluster = max(model[doc],key=lambda x:x[1])\n",
    "        sent_to_cluster.append(cluster[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from collections import Counter\n",
    "for book, cluster in book_id.items():\n",
    "    assignments = list()\n",
    "    for real,given in zip(chunk_class,sent_to_cluster):\n",
    "        if real == cluster:\n",
    "            assignments.append(given)\n",
    "    most_common,num_most_common = Counter(assignments).most_common(1)[0] # 4, 6 times\n",
    "    print(book,\":\",most_common,\"-\",num_most_common)\n",
    "    print(\"Accuracy:\",num_most_common/limit)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Semantic analysis based on lexical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what pre-build categories we can study! More information: https://hci.stanford.edu/publications/2016/ethan/empath-chi-2016.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in list(lexicon.cats.keys())[:15]:\n",
    "    print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each category, we can examine representative terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.cats[\"health\"][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying Pride and Prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(books[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_features = lexicon.analyze(doc.text,categories = [\"disappointment\", \"pain\", \"joy\", \"beauty\", \"affection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_features = lexicon.analyze(doc.text,categories = [\"disappointment\", \"pain\", \"joy\", \"beauty\", \"affection\"], normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usecase: the evolution of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(0,len(doc.text),150000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "love = []\n",
    "pain = []\n",
    "beauty = []\n",
    "affection = []\n",
    "\n",
    "\n",
    "for cnt,i in enumerate(bins[:-1]):\n",
    "    empath_features = lexicon.analyze(doc.text[bins[cnt]:bins[cnt+1]],\n",
    "                                      categories = [\"love\", \"pain\", \"joy\", \"beauty\", \"affection\"], normalize = True)\n",
    "    love.append(empath_features[\"love\"])\n",
    "    pain.append(empath_features[\"pain\"])\n",
    "    beauty.append(empath_features[\"beauty\"])\n",
    "    affection.append(empath_features[\"affection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(love,label = \"love\")\n",
    "plt.plot(beauty, label = \"beauty\")\n",
    "plt.plot(affection, label = \"affection\")\n",
    "plt.plot(pain,label = \"pain\")\n",
    "\n",
    "plt.xlabel(\"progression in the book\")\n",
    "plt.ylabel(\"frequency of a category\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can observe how as the story progresses, beauty and affection categories decrease, while love and pain intensify!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can create custom categories based on seed terms!\n",
    "### Models trained on: fiction, nytimes or reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.create_category(\"healthy_food\", [\"healthy_food\",\"low_carb\",\"kale\",\"avocado\"], model=\"nytimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.create_category(\"healthy_food\", [\"healthy_food\",\"low_carb\",\"kale\",\"avocado\"], model=\"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.create_category(\"festive_food\", [\"festive_food\",\"turkey\",\"eggnog\"], model=\"nytimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of further resources\n",
    "    * NLTK\n",
    "    * Jellyfish: string similarity library\n",
    "    * TextBlob: simplified text processing\n",
    "    * PyEnchant: language detection\n",
    "    * WordNet: ontology of concepts (inc. in NLTK)\n",
    "    * Stanford NLP (Java)\n",
    "    * Tweet NLP: https://www.cs.cmu.edu/~ark/TweetNLP/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "An NLP preprocessing model, using the list of stopwords ['a', 'the', 'is', 'you', 'I','my','with','was'] for the input sentence\n",
    "“I was driving down the street with my CAR! :)” <br>\n",
    "gives the following output: {driving, down, street, car, :)}.\n",
    "The model consists of: <br>\n",
    "\n",
    "a) Stopword Removal, Casefolding and Stemming <br>\n",
    "b) Stopword Removal and Casefolding <br>\n",
    "c) Stopword Removal and Stemming <br>\n",
    "d) Casefolding and Stemming <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code and/or answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "What statement is false about topic detection using LSA: <br>\n",
    "\n",
    "a) It finds the representation of documents and words in the latent \"topic space\" <br>\n",
    "b) It produces topics interpretable in “word space” <br>\n",
    "c) A document's topic representation is a probability distribution over topics <br>\n",
    "d) The vectors representing topics in “word space” are linearly independent <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code and/or answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "The problem of having more features than documents when using a TF-IDF matrix for document classification cannot be addressed by: <br>\n",
    "\n",
    "a) Using SVD for dimensionality reduction <br>\n",
    "b) Using min-max scaling of features <br>\n",
    "c) Penalizing very large positive and very large negative weights (a.k.a. regularization) <br>\n",
    "d) Using online feature selection <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code and/or answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Which of the following is false about word vectors: <br>\n",
    "\n",
    "a) Word2vec is used to achieve a lower dimensional representation than bag of words <br>\n",
    "b) Semantically similar words typically have similar word2vec vectors <br>\n",
    "c) Each dimension in the word2vec embedding has a clear interpretation <br>\n",
    "d) Word2Vec can be used for the computation of sentence vectors <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code and/or answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "Given the following word (W) / context (C) co-occurrence frequencies, which statement is true regarding the pointwise mutual information (PMI)?\n",
    "\n",
    "|  |W1|W2|W3|\n",
    "|--|--|--|--|\n",
    "|C1| 2| 0| 3|\n",
    "|C2| 6| 4| 0|\n",
    "|C3| 1| 1| 3|\n",
    "\n",
    "a) PMI(w=W1, c=C2) > PMI(w=W2, c=C2) > PMI(w=W3, c=C3) <br>\n",
    "b) PMI(w=W2, c=C2) > PMI(w=W1, c=C2) >  PMI(w=W3, c=C3) <br>\n",
    "c) PMI(w=W3, c=C3) > PMI(w=W1, c=C2) > PMI(w=W2, c=C2) <br>\n",
    "d) PMI(w=W3, c=C3) > PMI(w=W2, c=C2) > PMI(w=W1, c=C2) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code and/or answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "228px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
