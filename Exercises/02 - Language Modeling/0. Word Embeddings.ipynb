{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2252f27a",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "561fcd74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T21:45:06.720910Z",
     "start_time": "2025-09-11T21:45:05.470117Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import tokenize, simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import BinaryAccuracy"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "fa4427ed",
   "metadata": {},
   "source": [
    "## Exercise 1: Training a Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97858931",
   "metadata": {},
   "source": [
    "### Preparing the Corpus and Stopwords\n",
    "\n",
    "To train our own **Word2Vec embeddings**, we first need a text corpus.  \n",
    "Here we‚Äôll use the **Brown Corpus**, a classic collection of English texts available through NLTK.  \n",
    "\n",
    "- We download the `brown` corpus and the list of common English **stopwords**.  \n",
    "- Stopwords (like *the*, *and*, *is*) carry little semantic meaning, so we‚Äôll filter them out before training.  \n",
    "\n",
    "This ensures our Word2Vec model focuses on more informative words when learning embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "89825ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T21:45:10.088097Z",
     "start_time": "2025-09-11T21:45:10.005642Z"
    }
   },
   "source": [
    "nltk.download(\"brown\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/richardzhang/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/richardzhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b82edba5",
   "metadata": {},
   "source": [
    "### Exploring the Brown Corpus\n",
    "\n",
    "Before training Word2Vec, let‚Äôs take a look at the data we‚Äôll be working with.  \n",
    "\n",
    "- The **Brown Corpus** is divided into categories (e.g., news, fiction, humor).  \n",
    "- We print the available categories and the total number of sentences in the corpus.  \n",
    "- For illustration, we also display the first few sentences so we can see how the raw data looks.  \n",
    "\n",
    "This step helps us understand the **structure and style of the text** before preprocessing and training embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5872758d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T21:45:40.611948Z",
     "start_time": "2025-09-11T21:45:39.855286Z"
    }
   },
   "source": [
    "# Show some info about the corpus\n",
    "print(\"Categories:\", brown.categories())\n",
    "print(\"Total sentences:\", len(brown.sents()))\n",
    "\n",
    "# Get 100 senteces from the humor category\n",
    "dataset = brown.sents()\n",
    "\n",
    "for i in range(3):\n",
    "    print(dataset[i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "Total sentences: 57340\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "89a00a5c",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing the Text\n",
    "\n",
    "Raw sentences from the Brown Corpus need to be **preprocessed** before they can be used for training Word2Vec.  \n",
    "\n",
    "Steps applied:  \n",
    "1. **Join and tokenize** each sentence into words.  \n",
    "2. **Lowercasing & deaccenting** (`deacc=True` removes punctuation/accents).  \n",
    "3. **Filter short words** (`min_len=2` keeps words with at least 2 characters).  \n",
    "4. **Remove stopwords** (like *the*, *and*, *is*) to focus on meaningful content.  \n",
    "\n",
    "The result is a list of cleaned, tokenized sentences.  \n",
    "We print a few examples to see how the preprocessing transforms the text.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:08:32.081196Z",
     "start_time": "2025-09-11T22:08:32.075101Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[0]\n",
   "id": "fcc9882cfceddc3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "e08fbee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:13:18.816178Z",
     "start_time": "2025-09-11T22:13:17.021826Z"
    }
   },
   "source": [
    "cleaned_sentences = []\n",
    "for sent in tqdm(dataset):\n",
    "    tokens = list(simple_preprocess(\" \".join(sent), deacc=True, min_len=2))\n",
    "    #tokens = list(tokenize(\" \".join(sent), deacc=True, lower=True, min_len=2))\n",
    "    filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "    cleaned_sentences.append(filtered_tokens)\n",
    "\n",
    "for i in range(3):\n",
    "    print(cleaned_sentences[i])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57340/57340 [00:01<00:00, 32035.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlanta', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n",
      "['jury', 'said', 'term', 'end', 'presentments', 'city', 'executive', 'committee', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted']\n",
      "['september', 'october', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard', 'fought', 'primary', 'mayor', 'nominate', 'ivan', 'allen', 'jr']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "bae961c5",
   "metadata": {},
   "source": [
    "### Training a Word2Vec Model\n",
    "\n",
    "Now we train our own **Word2Vec embeddings** on the Brown Corpus.  \n",
    "\n",
    "Key parameters:  \n",
    "- `vector_size=50`: each word is represented as a 50-dimensional vector.  \n",
    "- `window=3`: the model looks at 3 words to the left and right for context.  \n",
    "- `min_count=1`: keep all words (even rare ones).  \n",
    "- `sg=1`: use the **skip-gram** approach, which works well for smaller datasets.  \n",
    "- `epochs=20`: make multiple passes over the data to learn stronger embeddings.  \n",
    "\n",
    "After training, we can:  \n",
    "- Inspect the learned vector for a specific word (here, `\"engineer\"`).  \n",
    "- Find its **nearest neighbors** in the embedding space, i.e., words with similar meanings or usage contexts.  \n",
    "\n",
    "This demonstrates how Word2Vec captures **semantic similarity** directly from the text we trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e09af82f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:15:38.250486Z",
     "start_time": "2025-09-11T22:15:30.763225Z"
    }
   },
   "source": [
    "model = Word2Vec(\n",
    "    sentences=cleaned_sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    epochs=20,\n",
    "    workers=4,\n",
    ")\n",
    "\n",
    "word = \"jury\"\n",
    "# Inspect a word vector\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(model.wv[word][:10])   # show first 10 dimensions\n",
    "\n",
    "# Check nearest neighbors\n",
    "print(f\"\\nMost similar to '{word}':\")\n",
    "print(model.wv.most_similar(word, topn=5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'jury':\n",
      "[-0.25011435  0.10722679 -0.22888315  0.47401857 -0.5392159  -0.29314518\n",
      " -0.53833824  0.9110671  -0.31575522  0.2260558 ]\n",
      "\n",
      "Most similar to 'jury':\n",
      "[('wexler', 0.7897740006446838), ('ballot', 0.758169412612915), ('prosecution', 0.7549406290054321), ('karns', 0.7484284043312073), ('defendants', 0.7478192448616028)]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "9ce606d9",
   "metadata": {},
   "source": [
    "## Exercise 2: Using GloVe Embeddings to train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffc42d",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Word Embeddings\n",
    "\n",
    "Instead of learning word vectors from scratch, we can use **pre-trained embeddings** that capture semantic relationships between words.  \n",
    "Here we download the **GloVe embeddings** (Global Vectors for Word Representation) trained on a large Wikipedia + Gigaword corpus.  \n",
    "\n",
    "- Each word is mapped to a **100-dimensional vector**.  \n",
    "- Words that appear in similar contexts (e.g., *king* and *queen*) will have vectors that are close to each other in this space.  \n",
    "\n",
    "These embeddings will serve as the foundation for representing text in our binary classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7d4873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:16:27.534873Z",
     "start_time": "2025-09-11T22:16:00.281309Z"
    }
   },
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.3% 23.4/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 45.7% 58.5/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 72.6% 93.0/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 98.7% 126.4/128.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "bbe92640",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "For this example, we‚Äôll use the **Sentiment140 Twitter dataset**, which contains tweets labeled for **sentiment (positive or negative)**.  \n",
    "This is a common benchmark dataset for text classification tasks.  \n",
    "\n",
    "- We load the dataset using the ü§ó **`datasets`** library.  \n",
    "- To keep the demo lightweight and fast, we only take a **subset** of the data:  \n",
    "  - 50,000 tweets for training  \n",
    "  - 10,000 tweets for testing  \n",
    "\n",
    "Finally, we print the dataset sizes to confirm our selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "943edfd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:16:50.211869Z",
     "start_time": "2025-09-11T22:16:33.780148Z"
    }
   },
   "source": [
    "ds = load_dataset(\"adilbekovich/Sentiment140Twitter\")\n",
    "\n",
    "train = ds[\"train\"].select(range(50_000))\n",
    "test = ds[\"test\"].select(range(10_000))\n",
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1360000/1360000 [00:01<00:00, 1269347.50 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240000/240000 [00:00<00:00, 1332215.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50000\n",
      "Test set size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:43:44.152052Z",
     "start_time": "2025-09-11T22:43:44.146857Z"
    }
   },
   "cell_type": "code",
   "source": "train['label'][0].dtype",
   "id": "1e07076d865c9abf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "c694b0f4",
   "metadata": {},
   "source": [
    "### Converting Tweets into Embeddings\n",
    "\n",
    "Machine learning models can‚Äôt directly process raw text, so we need to convert each tweet into a **numeric vector**.  \n",
    "\n",
    "We define a function `sentence_embedding` that:  \n",
    "1. **Tokenizes** the sentence into words (removing punctuation and making everything lowercase).  \n",
    "2. Looks up the **GloVe vector** for each word.  \n",
    "3. Computes the **average** of all word vectors to create a single fixed-length representation of the entire sentence.  \n",
    "   - If a sentence has no known words, we assign a zero vector.  \n",
    "\n",
    "Then we:  \n",
    "- Apply this function to every tweet in the train and test sets.  \n",
    "- Store the result in a new column called `\"embeddings\"`.  \n",
    "- Format the dataset so that `\"embeddings\"` and `\"label\"` are ready to be used as **PyTorch tensors** for training a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd9a9a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:33:32.139865Z",
     "start_time": "2025-09-11T22:33:26.929926Z"
    }
   },
   "source": [
    "def sentence_embedding(sentence: str, model: Any) -> np.ndarray:\n",
    "    # TODO: implement this function\n",
    "    tokens = list(simple_preprocess(sentence, deacc=True, min_len=2))\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "train = train.map(lambda x: {'embeddings': sentence_embedding(x['text'], glove_vectors)})  \n",
    "# TODO: Map the training set to add embeddings\n",
    "test =  test.map(lambda x: {'embeddings': sentence_embedding(x['text'], glove_vectors)})\n",
    "# TODO: Map the test set to add embeddings\n",
    "\n",
    "train.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])\n",
    "test.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:02<00:00, 22251.11 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 21789.40 examples/s]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:33:49.644012Z",
     "start_time": "2025-09-11T22:33:49.640178Z"
    }
   },
   "cell_type": "code",
   "source": "train",
   "id": "fed9336cabf5f891",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'embeddings'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "c7999a34",
   "metadata": {},
   "source": [
    "### Building a Simple Neural Network Classifier\n",
    "\n",
    "Now that we have numeric embeddings for each tweet, we can train a **neural network** to classify sentiment.  \n",
    "\n",
    "We define a PyTorch model `SentimentClassifier` with the following structure:  \n",
    "\n",
    "1. **Input layer**: takes in the 100-dimensional GloVe embedding for each tweet.  \n",
    "2. **Hidden layer**: a fully connected layer with 256 units and a **ReLU activation**, which introduces non-linearity.  \n",
    "3. **Output layer**: a single neuron that predicts the probability of the tweet being **positive** (values between 0 and 1).  \n",
    "   - We use a **sigmoid activation** to squash the output.  \n",
    "\n",
    "This simple feed-forward network is powerful enough to learn sentiment patterns from our averaged word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2149cc50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:54:00.834931Z",
     "start_time": "2025-09-11T22:54:00.830149Z"
    }
   },
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Define the layers and the activation functions\n",
    "        self.fc = nn.Linear(input_dim, 256,bias=True)\n",
    "        self.output_layer = nn.Linear(256, 1,bias=True)\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        x = self.fc(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = torch.sigmoid(x).squeeze(1)\n",
    "        return x\n",
    "\n",
    "model = SentimentClassifier(input_dim=glove_vectors.vector_size)\n",
    "# TODO: Initialize the model with the correct input dimension (hint: extract dimension from glove_vectors)\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (fc): Linear(in_features=100, out_features=256, bias=True)\n",
       "  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "id": "e3c06dd9",
   "metadata": {},
   "source": [
    "### Setting Up Training Parameters\n",
    "\n",
    "Before training the model, we need to define some key components:  \n",
    "\n",
    "- **Batch size (64)**: the number of samples processed at once before updating the model‚Äôs parameters.  \n",
    "- **Epochs (30)**: how many times the model will see the entire training dataset.  \n",
    "- **Loss function**: we use **Binary Cross-Entropy Loss (`BCELoss`)**, which is standard for binary classification tasks.  \n",
    "- **Optimizer**: we use **Stochastic Gradient Descent (SGD)** with a learning rate of 0.01 to update model weights during training.  \n",
    "\n",
    "We also wrap our dataset into **DataLoaders**:  \n",
    "- `train_dataloader`: feeds batches of tweets into the model, shuffling to avoid order bias.  \n",
    "- `test_dataloader`: used for evaluation (no shuffling needed).  \n",
    "\n",
    "This setup prepares us for the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "682cf8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:54:05.783276Z",
     "start_time": "2025-09-11T22:54:05.779039Z"
    }
   },
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "loss_fn = nn.BCELoss()\n",
    "acc_fn = BinaryAccuracy()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "id": "007f2508",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "\n",
    "Now we train our sentiment classifier using the training data.  \n",
    "\n",
    "For each **epoch** (full pass over the training set):  \n",
    "1. **Batching**: the `DataLoader` gives us a batch of tweet embeddings and labels.  \n",
    "2. **Forward pass**: the embeddings are passed through the model to get predictions.  \n",
    "3. **Loss calculation**: compare predictions with the true labels using **binary cross-entropy**.  \n",
    "4. **Backward pass**: compute gradients of the loss with respect to model parameters.  \n",
    "5. **Optimizer step**: update model weights using **SGD**.  \n",
    "6. **Repeat** for all batches in the epoch.  \n",
    "\n",
    "At the end of each epoch, we print the **average training loss**, which shows how well the model is learning over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c49f9b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:54:33.505819Z",
     "start_time": "2025-09-11T22:54:07.531341Z"
    }
   },
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i,  batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Get the inputs and labels from the batch\n",
    "        inputs = batch['embeddings']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs) # TODO: Get model outputs\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(outputs,labels.float()) #TODO: Compute the loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        # TODO: Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(train_dataloader)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6866065859794617\n",
      "Epoch 2/30, Loss: 0.6698783040046692\n",
      "Epoch 3/30, Loss: 0.6497867703437805\n",
      "Epoch 4/30, Loss: 0.6297317743301392\n",
      "Epoch 5/30, Loss: 0.613480806350708\n",
      "Epoch 6/30, Loss: 0.6018441319465637\n",
      "Epoch 7/30, Loss: 0.5939953327178955\n",
      "Epoch 8/30, Loss: 0.5886243581771851\n",
      "Epoch 9/30, Loss: 0.5843974947929382\n",
      "Epoch 10/30, Loss: 0.5812150835990906\n",
      "Epoch 11/30, Loss: 0.5787811875343323\n",
      "Epoch 12/30, Loss: 0.5767233967781067\n",
      "Epoch 13/30, Loss: 0.5752143859863281\n",
      "Epoch 14/30, Loss: 0.5736380815505981\n",
      "Epoch 15/30, Loss: 0.5727952718734741\n",
      "Epoch 16/30, Loss: 0.5717468857765198\n",
      "Epoch 17/30, Loss: 0.5708679556846619\n",
      "Epoch 18/30, Loss: 0.5698680281639099\n",
      "Epoch 19/30, Loss: 0.5693554282188416\n",
      "Epoch 20/30, Loss: 0.5686982274055481\n",
      "Epoch 21/30, Loss: 0.5682315230369568\n",
      "Epoch 22/30, Loss: 0.5678175687789917\n",
      "Epoch 23/30, Loss: 0.5672368407249451\n",
      "Epoch 24/30, Loss: 0.5665106177330017\n",
      "Epoch 25/30, Loss: 0.5662782788276672\n",
      "Epoch 26/30, Loss: 0.5657272934913635\n",
      "Epoch 27/30, Loss: 0.5653555393218994\n",
      "Epoch 28/30, Loss: 0.5651248693466187\n",
      "Epoch 29/30, Loss: 0.5645033717155457\n",
      "Epoch 30/30, Loss: 0.5643495917320251\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f4f26d32875d3f38"
  },
  {
   "cell_type": "markdown",
   "id": "afe9bbef",
   "metadata": {},
   "source": [
    "### Evaluating the Model on Test Data\n",
    "\n",
    "After training, we switch the model to **evaluation mode** (`model.eval()`) to test its performance.  \n",
    "\n",
    "For each batch in the test set:  \n",
    "1. **Forward pass**: compute predictions for the embeddings.  \n",
    "2. **Loss calculation**: measure how far predictions are from the true labels.  \n",
    "3. **Thresholding**: since outputs are probabilities (0‚Äì1), we assign  \n",
    "   - `1` if prediction > 0.5 (positive sentiment)  \n",
    "   - `0` otherwise (negative sentiment).  \n",
    "4. **Accuracy**: compare predictions with labels to compute the percentage of correct classifications.  \n",
    "\n",
    "Finally, we average the results across all batches and print the **test accuracy**, which tells us how well the model generalizes to unseen tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "32a26fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:54:37.565359Z",
     "start_time": "2025-09-11T22:54:37.373234Z"
    }
   },
   "source": [
    "batch_loss = 0\n",
    "batch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "for i,  batch in enumerate(test_dataloader):\n",
    "    # TODO: Load the inputs and the labels from the batch, then run the forward pass, compute loss, predictions and accuracy\n",
    "    # get the inputs\n",
    "    inputs = batch['embeddings']\n",
    "    labels = batch['label']\n",
    "    \n",
    "    # run forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    test_loss = loss_fn(outputs, labels.float())\n",
    "\n",
    "    predictions = (torch.where(outputs > 0.5, 1, 0)).int()\n",
    "    \n",
    "    \n",
    "    acc = acc_fn(predictions, labels)\n",
    "    \n",
    "    batch_loss += test_loss.item()\n",
    "    batch_acc += acc.item()\n",
    "\n",
    "test_loss = batch_loss / len(test_dataloader)\n",
    "test_acc = batch_acc / len(test_dataloader)\n",
    "\n",
    "print(f'Test Acc: {test_acc*100:.2f}%')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 71.05%\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "id": "96f84ad6",
   "metadata": {},
   "source": [
    "## 3. MCQ\n",
    "\n",
    "### 3.1. Purpose of Word Embeddings\n",
    "\n",
    "What is the main purpose of word embeddings in NLP?\n",
    "\n",
    "A. To convert words into high-dimensional one-hot vectors<br>\n",
    "B. To map words into continuous vector spaces that capture semantic meaning<br>\n",
    "C. To remove stopwords from text before processing<br>\n",
    "D. To reduce the training time of convolutional networks<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. One-Hot vs. Embeddings\n",
    "\n",
    "Compared to one-hot encoding, word embeddings:\n",
    "\n",
    "A. Have the same dimensionality as the vocabulary size<br>\n",
    "B. Provide dense, low-dimensional representations that capture similarities<br>\n",
    "C. Are always manually designed by experts<br>\n",
    "D. Cannot be trained with neural networks<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.3. Word2Vec Models\n",
    "\n",
    "The Skip-gram model in Word2Vec is designed to:\n",
    "\n",
    "A. Predict the context words given a target word<br>\n",
    "B. Predict the target word given the context words<br>\n",
    "C. Cluster words into fixed categories<br>\n",
    "D. Remove rare words from the corpus<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.4. Embedding Matrix Shape\n",
    "\n",
    "In a neural network with vocabulary size $V$ and embedding dimension $d$, the embedding matrix has shape:\n",
    "\n",
    "A. $(d \\times V)$ <br>\n",
    "B. $(V \\times d)$<br>\n",
    "C. $(V \\times V)$<br>\n",
    "D. $(d \\times d)$<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.5. Semantic Relationships\n",
    "\n",
    "Word embeddings can capture analogies such as:\n",
    "\n",
    "A. king ‚Äì man + woman ‚âà queen<br>\n",
    "B. dog ‚Äì cat + car ‚âà airplane<br>\n",
    "C. apple ‚Äì red + fast ‚âà running<br>\n",
    "D. chair ‚Äì table + sky ‚âà cloud<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6. Contextual vs. Static Embeddings\n",
    "\n",
    "How do contextual embeddings (e.g., BERT) differ from static embeddings (e.g., Word2Vec)?\n",
    "\n",
    "A. They assign the same vector to a word regardless of context<br>\n",
    "B. They assign different vectors to a word depending on its context<br>\n",
    "C. They are always lower-dimensional than static embeddings<br>\n",
    "D. They do not require pretraining on large corpora<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.7. Sparse vs. Dense Representations\n",
    "\n",
    "Compared to Bag-of-Words (BoW) vectors, neural word embeddings are:\n",
    "\n",
    "A. Higher dimensional and sparse<br>\n",
    "B. Always binary representations<br>\n",
    "C. Lower dimensional and sparse<br>\n",
    "D. Lower dimensional and dense<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.8. Semantic Similarity\n",
    "\n",
    "What is the main advantage of word embeddings over one-hot encodings?\n",
    "\n",
    "A. They guarantee perfect accuracy in classification tasks<br>\n",
    "B. They eliminate the need for training neural networks<br>\n",
    "C. They capture semantic similarity between words in vector space<br>\n",
    "D. They automatically remove stopwords from text<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.9. CBOW vs. Skip-gram\n",
    "\n",
    "The Continuous Bag of Words (CBOW) model aims to:\n",
    "\n",
    "A. Predict the target word given its surrounding context words<br>\n",
    "B. Assign unique one-hot vectors to words<br>\n",
    "C. Predict the context words given a target word<br>\n",
    "D. Cluster words into topics using SVD<br>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### 3.10. Distributional Semantics\n",
    "\n",
    "The idea that ‚Äúyou shall know a word by the company it keeps‚Äù refers to:\n",
    "\n",
    "A. Overfitting in NLP models<br>\n",
    "B. Context-based learning of embeddings<br>\n",
    "C. Sentence segmentation<br>\n",
    "D. Stopword removal<br>\n",
    "\n",
    "**Answer:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
